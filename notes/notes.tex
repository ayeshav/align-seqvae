\section{Problem Statement}

What do we want? A fast and data efficient way to combine observations collected across different sessions, animals 
such that they can be used to learn a common, general dynamics model. Let's consider the following generative model:

\begin{center}
$\vx \sim p_{\phi}(\vx)$

$\vy|\vx \sim p(\vy|g_{\theta}(\vz))$
\end{center}

Owing to the popularity of VAEs and their wide success at inferring neural dynamics, let's focus on them for now.

\begin{center}
$ \mathcal{L}(\theta, \psi, \varphi) = \sum_{i-1}^N \log p_{\theta}(\vx_i|\vz_i) + \log p_{\psi}(\vz_i) - \log q_{\varphi} (\vz_i|\vx_i)$
\end{center}

Despite their popularity, there are some issues with doing inference using VAEs. 
\begin{enumerate}
\item{\textbf{Online learning.} With each new sample, the decoder parameters $\theta$ are updated. And every value of $\theta$ leads to a different latent space, making online analysis harder to interpret.}
\item{\textbf{Posterior collapse.} Models that have expressive likelihoods, and require learning a prior, for instance, in state space models, it is common for the prior, $\psi$, to be untrained. This is because the likelihood term dominates the ELBO. If the $theta$ is fixed, the ELBO can only change by finding a good latent representation.} 
\end{enumerate}

The main idea is simple -- we fix the likelihood parameter $\theta$ and learn the dynamics. This will help in addressing the aforementioned issues. Moreover, this will allows us to take advantage of multiple instances of data with the same underlying dynamics and would (hopefully) allow us to learn a more general dynamical model. Some popular LVMs for neuroscience, example LFADS, CEBRA learn a dynamics model by 
combining data across different sessions. However, in order to incorporate new data into such a framework, these models have to be re-trained along with an embedding layer for the new dataset. Using our approach, we can directly infer the dynamics for new data by recovering a map between our observations. Specifically, it's easy to include an alignment function for new datasets such that the VAE parameters can be 
re-used without having to re-train them. This would offer a huge benefit for online inference tasks. 

\section{Model description}
Let's say we have neural data from multiple observation spaces $Y_1, Y_2, ... Y_m \in \mathbb{R}^{d_1}, \mathbb{R}^{d_2}, ... ,\mathbb{R}^{d_m}$ and we assume that they have the same underlying dynamics $X \in \mathbb{R}^{d_x}$, where $d_x \ll d_1, d_2, ..., d_m$. We want to learn a bijective map between the observation spaces.  


\subsection{Generative model}

\subsubsection{The base VAE}

For simplicity, let's pick a reference observation space $Y_\text{ref}$ and train a base sequence VAE model. 
\begin{center}
    $p(\vy_{\text{ref},1:T}, \vx_{1:T}) = p(\vx_1) \prod_{2}^{T}p(\vy_{\text{ref},t}|\vx_t) p(\vx_t|\vx_{t-1})$

    $p(\vy_{\text{ref},t}|\vx_t) = \mathcal{N}(C\vx_t, Q)$

    $p(\vx_t|\vx_{t-1}) = \mathcal{N} (\text{NN}_{\theta} (\vx_{t-1}), \text{NN}_{\theta} (\vx_{t-1}))$

    $q_{\varphi}(\vx_t|\vy_{\text{ref},t}) = q_{\varphi}(\vx_1|\vy_{\text{ref},1:T}) \prod_{t=2}^{T} q_{\varphi} (\vx_t | \vy_{\text{ref},t:T}) $

\end{center}

where $C, Q, \theta, \varphi$ are the model parameters. 

\subsubsection{Alignment across observation spaces}

Once we have the base VAE model, we will align the observation spaces. In the simple cases, where we have the same number of trials and matching/similar trial lengths, aligning is straightforward. In such a case, aligning is equivalent to learning a linear map such that $ \underset{W_i}{\argmin} ||Y_iW_i - Y_1|| $. 

However in realistic settings, this is not the case. For each observation space, we have a different number of trials and trial lengths. A naive approach could be to consider condition averaged time series to allow learning such a linear map. This approach has been shown to not work very well. Plus, it doesn't take advantage of the fact that 
these observations have shared low-dimensional dynamics. Instead, we will train $g_{\alpha}$ to maximize the likelihood of the observations in the new space.

\begin{center}
    $p(\vy_{i,1:T}, \vx_{1:T}) = p(\vy_{i,1:T}|\vy_{\text{ref},1:T}) p(\vx_1) \prod_{2}^{T}p(\vy_{\text{ref},t}|\vx_t) p(\vx_t|\vx_{t-1}) $

    $p(\vy_{i,1:T}|\vy_{\text{ref},1:T}) = g_{\alpha}^{-1}(\vy_{\text{ref}, 1:T})$
\end{center}

Here, we propose to learn a function $g_{\alpha} : \mathbb{R}^{d_i} \to \mathbb{R}^{d_{\text{ref}}}$ that defines a map between different observation spaces. The parameter mapping latents to the \text{ref}erence space remain fixed. The parameter $\theta$ of the prior can be optionally updated for improvement in training.

$g_{\alpha}$ should ideally define a diffeomorphism between the observation spaces and the reference space, i.e., it should be a bijection and invertible. 

Considering the 


\subsection{Inference}

For inferring the parameters of the base VAE, we will maximize the ELBO:

$$\mathcal{L_{\text{ELBO}} (C, Q, \theta, \varphi)} = \sum_{t}\log p(\vy_{\text{ref}, t}|\vx_t) + \mathbb{D}_{\text{KL}}[q_{\varphi}(\vx_t|\vy_{\text{ref}, t})\: || \:p_{\theta}(\vx)]$$

After getting a reference VAE, we will train the alignment function such that it maximizes the likelihood of new data:

$$\mathcal{L_A} (\alpha, \theta) = \log p(\vy_i|q_{\psi}(\vy_{\text{ref}})) + \log p_{\theta}(\vx|q_{\psi}(\vy_{\text{ref}}))$$

\section{Test Case}

Let's consider a simple test case where $g_{\alpha}$ is a linear map. 



\subsection{Related Work}
Alternative approaches, and methods we should compare to. We will (hopefully) have an advantage in low data and streaming settings. 

\begin{enumerate}
    \item Dynamical stitiching in LFADS, CEBRA.
    \item Learning a new model for each dataset.
    \item Learning an embedding from latents to observations directly.
\end{enumerate}



% \subsection{Challenges}
